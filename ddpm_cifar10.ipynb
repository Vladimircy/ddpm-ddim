{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kAkGZn4ncF1N"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LoUQD1vXcF1Q"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "class Swish(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    def forward(self,x):\n",
        "        return x * torch.sigmoid(x)\n",
        "class TimestampEmbedding(nn.Module):\n",
        "    def __init__(self,T,hid_dim,out_dim):\n",
        "        '''hid_dim must be even'''\n",
        "        assert hid_dim % 2 == 0\n",
        "        super().__init__()\n",
        "        self.T = T\n",
        "        self.hid_dim = hid_dim\n",
        "        self.out_dim = out_dim\n",
        "        emb = torch.zeros((T,hid_dim))\n",
        "        t = torch.arange(0,T).float()\n",
        "        pos = torch.exp(- torch.arange(0,hid_dim,2).float() * math.log(10000) / hid_dim)\n",
        "        pos = pos[None,:] * t[:,None]\n",
        "        emb[:,0::2] = torch.sin(pos)\n",
        "        emb[:,1::2] = torch.cos(pos)\n",
        "        assert list(emb.shape) == [T,hid_dim], emb.shape\n",
        "        self.emb_layer = nn.Embedding.from_pretrained(emb)\n",
        "        self.linear_projection = nn.Sequential(\n",
        "            nn.Linear(hid_dim,hid_dim),\n",
        "            Swish(),\n",
        "            nn.Linear(hid_dim,out_dim),\n",
        "        )\n",
        "    def forward(self,t):\n",
        "        ts = self.emb_layer(t)\n",
        "        return self.linear_projection(ts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-1.0124e-01, -1.8524e-01, -7.5046e-02,  1.2864e-01, -1.2727e-01,\n",
              "          1.2264e-01,  9.7239e-02, -5.3639e-02,  3.0916e-01,  3.1185e-02,\n",
              "          5.8693e-02, -9.3891e-02,  9.7069e-02, -2.3483e-02, -4.5807e-02,\n",
              "          1.5918e-04, -8.8223e-02,  2.3478e-01,  4.4465e-02, -4.0098e-02,\n",
              "         -2.4875e-02,  2.7891e-02,  1.0597e-01, -1.6713e-01,  9.6536e-02,\n",
              "          1.0362e-02,  4.8702e-02,  2.2652e-02, -1.5555e-02,  4.8726e-02,\n",
              "         -7.6452e-02, -8.5779e-02, -1.3418e-01,  1.0486e-01, -2.1896e-02,\n",
              "         -1.8625e-01,  1.1470e-01, -1.4839e-01, -1.7249e-01,  1.1505e-01,\n",
              "         -2.0230e-02, -1.9516e-01, -2.3811e-02, -2.8151e-01,  6.4190e-02,\n",
              "          1.0873e-01,  1.7719e-01, -8.4682e-02, -7.2578e-02, -1.8599e-01,\n",
              "         -1.5161e-01, -2.7676e-01,  3.4338e-01,  2.8707e-01,  1.5990e-01,\n",
              "         -9.9030e-02, -6.5463e-02, -1.1371e-01, -3.7894e-02,  3.2099e-01,\n",
              "          1.4604e-01, -3.5411e-03,  1.0791e-01,  6.8421e-02]],\n",
              "       grad_fn=<AddmmBackward0>)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# test timeembedding\n",
        "T = 1000\n",
        "t_hid = 256\n",
        "t_out = 64\n",
        "temb = TimestampEmbedding(T,t_hid,t_out)\n",
        "temb(torch.tensor([100]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RYbJGSZ8cF1R"
      },
      "outputs": [],
      "source": [
        "class AttentionBlock(nn.Module):\n",
        "    '''\n",
        "    Self Attention block\n",
        "    x.shape = [B,C,H,W] -> group_norm. q,k,v = linear projection of x,\n",
        "    out = linear projection of softmax(q * k^T / d^0.05) * v\n",
        "    '''\n",
        "    def __init__(self,in_channel,num_heads,num_groups=4,hid_dim=None):\n",
        "        super().__init__()\n",
        "        self.groupnorm = nn.GroupNorm(num_groups,in_channel)\n",
        "        if hid_dim is None:\n",
        "            self.hid_dim = in_channel\n",
        "        else :\n",
        "            self.hid_dim = hid_dim\n",
        "        self.qkv_projection = nn.Conv2d(in_channel,3*num_heads*self.hid_dim,1,1,0)\n",
        "        self.out_projection = nn.Conv2d(num_heads*self.hid_dim,in_channel,1,1,0)\n",
        "        self.num_heads = num_heads\n",
        "    def forward(self,x):\n",
        "        B,_,H,W = x.shape\n",
        "        qkv = self.qkv_projection(x)\n",
        "        q,k,v = torch.chunk(qkv,3,dim=1)\n",
        "        q = q.reshape((B*self.num_heads,self.hid_dim,H*W)).permute(0,2,1)\n",
        "        k_t = k.reshape((B*self.num_heads,self.hid_dim,H*W))\n",
        "        v = v.reshape((B*self.num_heads,self.hid_dim,H*W)).permute(0,2,1)\n",
        "        attention_weight = nn.functional.softmax(torch.bmm(q,k_t) * self.hid_dim ** -0.5 ,dim=-1)\n",
        "        out = torch.bmm(attention_weight,v)\n",
        "        assert list(out.shape) == [B*self.num_heads,H*W,self.hid_dim]\n",
        "        out = out.reshape((B,self.num_heads,H*W,self.hid_dim)).permute(0,1,3,2)\n",
        "        out = out.reshape((B,self.num_heads*self.hid_dim,H,W))\n",
        "        return self.out_projection(out) + x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "n_tLK9HncF1S"
      },
      "outputs": [],
      "source": [
        "class ResBlock(nn.Module):\n",
        "    def __init__(self,in_channel,out_channel,t_out,dropout=0.5,num_group=4):\n",
        "        super().__init__()\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.GroupNorm(num_group,in_channel),\n",
        "            Swish(),\n",
        "            nn.Conv2d(in_channel,out_channel,3,1,1),\n",
        "        )\n",
        "        self.tsEmb_projection = nn.Sequential(\n",
        "            Swish(),\n",
        "            nn.Linear(t_out,out_channel),\n",
        "        )\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.GroupNorm(num_group,out_channel),\n",
        "            Swish(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Conv2d(out_channel,out_channel,3,1,1),\n",
        "        )\n",
        "        if in_channel != out_channel :\n",
        "            self.in2out = nn.Conv2d(in_channel,out_channel,3,1,1)\n",
        "        else :\n",
        "            self.in2out = nn.Identity()\n",
        "    def forward(self,x,tsEmb):\n",
        "        h = self.block1(x)\n",
        "        tsEmb = self.tsEmb_projection(tsEmb)[:,:,None,None]\n",
        "        h += tsEmb\n",
        "        h = self.block2(h)\n",
        "        h += self.in2out(x)\n",
        "        return h\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "O5HhcvO5cF1S"
      },
      "outputs": [],
      "source": [
        "class DownBlock(nn.Module):\n",
        "    def __init__(self,in_channel,out_channel,t_out,num_heads=4,dropout=0.5,num_group=4,has_attention=True):\n",
        "        super().__init__()\n",
        "        self.resblock = ResBlock(in_channel,out_channel,t_out,dropout,num_group)\n",
        "        if has_attention:\n",
        "            self.attention = AttentionBlock(out_channel,num_heads,num_group)\n",
        "        else:\n",
        "            self.attention = nn.Identity()\n",
        "    def forward(self,x,tEmb):\n",
        "        x = self.resblock(x,tEmb)\n",
        "        x = self.attention(x)\n",
        "        return x\n",
        "class UpBlock(nn.Module):\n",
        "    def __init__(self,in_channel,out_channel,t_out,num_heads=4,dropout=0.5,num_group=4,has_attention=True):\n",
        "        super().__init__()\n",
        "        self.resblock = ResBlock(in_channel,out_channel,t_out,dropout,num_group)\n",
        "        if has_attention:\n",
        "            self.attention = AttentionBlock(out_channel,num_heads,num_group)\n",
        "        else:\n",
        "            self.attention = nn.Identity()\n",
        "    def forward(self,x,tEmb):\n",
        "        x = self.resblock(x,tEmb)\n",
        "        x = self.attention(x)\n",
        "        return x\n",
        "class UpSample(nn.Module):\n",
        "    def __init__(self,in_channel):\n",
        "        super().__init__()\n",
        "        self.upsample = nn.Upsample(scale_factor=2,mode='bilinear',align_corners=False)\n",
        "        self.conv = nn.Conv2d(in_channel,in_channel//2,3,1,1)\n",
        "    def forward(self,x,tEmb):\n",
        "        _ = tEmb\n",
        "        x = self.conv(self.upsample(x))\n",
        "        return x\n",
        "class DownSample(nn.Module):\n",
        "    def __init__(self,in_channel):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channel,in_channel,3,2,1)\n",
        "    def forward(self,x,tEmb):\n",
        "        _ = tEmb\n",
        "        return self.conv(x)\n",
        "class MiddleBlock(nn.Module):\n",
        "    def __init__(self,in_channel,out_channel,t_out,num_heads=4,drop_out=0.5,num_group=4):\n",
        "        super().__init__()\n",
        "        self.resblock1 = ResBlock(in_channel,in_channel,t_out,drop_out,num_group)\n",
        "        self.resblock2 = ResBlock(in_channel,out_channel,t_out,drop_out,num_group)\n",
        "        self.attention = AttentionBlock(in_channel,num_heads,num_group)\n",
        "    def forward(self,x,tEmb):\n",
        "        x = self.resblock1(x,tEmb)\n",
        "        x = self.attention(x)\n",
        "        x = self.resblock2(x,tEmb)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "k2HjJm2acF1T"
      },
      "outputs": [],
      "source": [
        "#write a Unet using the above blocks\n",
        "\n",
        "class Conv(nn.Module):\n",
        "    def __init__(self,in_channel,out_channel,kernel,stride,padding) -> None:\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channel,out_channel,kernel,stride,padding)\n",
        "    def forward(self,x,tEmb):\n",
        "        _ = tEmb\n",
        "        return self.conv(x)\n",
        "class Unet(nn.Module):\n",
        "    def __init__(self,config:dict\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        steps = config['steps']\n",
        "        t_hid = config['t_hid']\n",
        "        t_out = config['t_out']\n",
        "        in_channel = config['in_channel']\n",
        "        self.timestamp_embedding = TimestampEmbedding(steps,t_hid,t_out)\n",
        "        input_process = []\n",
        "        for ch in config['input_process']:\n",
        "            if in_channel % config['num_group'] != 0:\n",
        "                input_process.append(Conv(in_channel,ch,3,1,1))\n",
        "            else :\n",
        "                input_process.append(DownBlock(in_channel,ch,t_out,has_attention=False))\n",
        "            in_channel = ch\n",
        "        self.input_process = nn.ModuleList(input_process)\n",
        "        downs = []\n",
        "        for down_ch in config['down_blocks']:\n",
        "            downs.append(DownSample(in_channel))\n",
        "            for ch in down_ch:\n",
        "                downs.append(DownBlock(in_channel,ch,t_out,num_heads=config[\"num_heads\"]))\n",
        "                in_channel = ch\n",
        "        self.downs = nn.ModuleList(downs)\n",
        "        middle_blocks = [DownSample(in_channel)]\n",
        "        for mid_ch in config['middle_blocks']:\n",
        "            middle_blocks.append(MiddleBlock(in_channel,mid_ch,t_out))\n",
        "            in_channel = mid_ch\n",
        "        middle_blocks.append(UpSample(in_channel))\n",
        "        self.middle_blocks = nn.ModuleList(middle_blocks)\n",
        "        up_blocks = []\n",
        "        for up_ch in config['up_blocks']:\n",
        "            for ch in up_ch:\n",
        "                up_blocks.append(UpBlock(in_channel,ch,t_out,num_heads=config[\"num_heads\"]))\n",
        "                in_channel = ch\n",
        "            up_blocks.append(UpSample(in_channel))\n",
        "        self.up_blocks = nn.ModuleList(up_blocks)\n",
        "        output_process = []\n",
        "        for ch in config['output_process']:\n",
        "            output_process.append(DownBlock(in_channel,ch,t_out))\n",
        "            in_channel = ch\n",
        "        output_process.append(Conv(in_channel,config['out_channel'],3,1,1))\n",
        "        self.output_process = nn.ModuleList(output_process)\n",
        "    def forward(self,x,t):\n",
        "        tsEmb = self.timestamp_embedding(t)\n",
        "        for input_process in self.input_process:\n",
        "            x = input_process(x,tsEmb)\n",
        "        downs = []\n",
        "        for down in self.downs:\n",
        "            if isinstance(down,DownSample):\n",
        "                downs.append(x)\n",
        "            x = down(x,tsEmb)\n",
        "        h = x\n",
        "        for middle in self.middle_blocks:\n",
        "            h = middle(h,tsEmb)\n",
        "        x = torch.cat([x,h],dim=1)\n",
        "        for up in self.up_blocks:\n",
        "            x = up(x,tsEmb)\n",
        "            if isinstance(up,UpSample):\n",
        "                down = downs.pop()\n",
        "                x = torch.cat([x,down],dim=1)\n",
        "        for output in self.output_process:\n",
        "            x = output(x,tsEmb)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NrpZN5jLcF1T"
      },
      "outputs": [],
      "source": [
        "def extract(origin,index,length):\n",
        "    res = torch.gather(origin,index=index,dim=0)\n",
        "    B = index.shape[0]\n",
        "    return res.reshape([B] + [1] * (length-1))\n",
        "class DDPM(nn.Module):\n",
        "    def __init__(self,Config):\n",
        "        super().__init__()\n",
        "        self.T = Config['steps']\n",
        "        beta_1 = Config['beta_1']\n",
        "        beta_T = Config['beta_t']\n",
        "        self.unet = Unet(Config)\n",
        "        betas = torch.linspace(beta_1,beta_T,self.T)\n",
        "        alphas = 1 - betas\n",
        "        alphas_bar = torch.cumprod(alphas,dim=0)\n",
        "        self.register_buffer(\"betas\",betas)\n",
        "        self.register_buffer(\"alphas\",alphas)\n",
        "        self.register_buffer(\"alphas_bar\",alphas_bar)\n",
        "        self.register_buffer(\"sqrt_alphas_bar\",torch.sqrt(alphas_bar))\n",
        "        self.register_buffer(\"sqrt_oneminus_alphas_bar\",torch.sqrt(1-alphas_bar))\n",
        "        self.register_buffer(\"sqrt_recip_alphas\", torch.sqrt(1/alphas))\n",
        "        self.register_buffer(\"noise_coeff\",self.betas/self.sqrt_oneminus_alphas_bar)\n",
        "    def forward(self,x):\n",
        "        device = next(self.parameters()).device\n",
        "        length = len(x.shape)\n",
        "        noise = torch.randn_like(x).to(device)\n",
        "        t = torch.randint(self.T,(x.shape[0],)).to(device)\n",
        "        x_t =  extract(self.sqrt_alphas_bar,t,length) * x + noise * extract(self.sqrt_oneminus_alphas_bar,t,length)\n",
        "        predicted_noise = self.unet(x_t,t)\n",
        "        loss = nn.functional.mse_loss(predicted_noise,noise)\n",
        "        return loss\n",
        "    def gen_noise_img(self,x_0,t):\n",
        "        assert t < self.T\n",
        "        device = next(self.parameters()).device\n",
        "        length = len(x_0.shape)\n",
        "        noise = torch.randn_like(x_0).to(device)\n",
        "        t = torch.full((x_0.shape[0],),t).to(device)\n",
        "        return extract(self.sqrt_alphas_bar,t,length) * x_0 + noise * extract(self.sqrt_oneminus_alphas_bar,t,length),noise\n",
        "    @torch.no_grad()\n",
        "    def sample(self,x,return_progress=False,step=None):\n",
        "        assert not return_progress or step != None,\"step must be given when return_progress is True\"\n",
        "        B = x.shape[0]\n",
        "        device = next(self.parameters()).device\n",
        "        length = len(x.shape)\n",
        "        x_t = x\n",
        "        res = x\n",
        "        for t in reversed(range(self.T)):\n",
        "            timesteps = torch.full((B,),t)\n",
        "            if t > 0 :\n",
        "                noise = torch.randn_like(x).to(device)\n",
        "            else :\n",
        "                noise = 0\n",
        "            timesteps = timesteps.to(device)\n",
        "            predicted_noise = self.unet(x_t,timesteps)\n",
        "            mean =  extract(self.sqrt_recip_alphas,timesteps,length) * (x_t - extract(self.noise_coeff,timesteps,length) * predicted_noise)\n",
        "            var = noise * torch.sqrt(extract(self.betas,timesteps,length))\n",
        "            x_t = mean + var\n",
        "            if return_progress and t % step == 0:\n",
        "                res = torch.cat([res,x_t],dim=0)\n",
        "        if return_progress:\n",
        "            return res.clamp(-1,1)\n",
        "        return  x_t.clamp(-1,1) \n",
        "# test DDPM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1eRyEcscF1U",
        "outputId": "2fa8feef-6f8d-4e8b-d036-e5f01ed85912"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "Config = {\n",
        "    \"steps\" : 1000,\n",
        "    \"beta_1\" : 1e-4,\n",
        "    \"beta_t\" : 0.02,\n",
        "    \"t_hid\" : 256,\n",
        "    \"t_out\" : 64,\n",
        "    \"in_channel\" : 3,\n",
        "    'out_channel' : 3,\n",
        "    \"num_heads\" : 2,\n",
        "    \"dropout\" : 0.5,\n",
        "    \"num_group\" : 32,\n",
        "    \"input_process\" : [\n",
        "        64,64\n",
        "    ],\n",
        "    \"output_process\" : [\n",
        "        64,64\n",
        "    ],\n",
        "    \"down_blocks\" : [\n",
        "        [128,128],\n",
        "        [256,256],\n",
        "        [512,512],\n",
        "    ] ,\n",
        "    \"middle_blocks\" :[\n",
        "        1024,1024\n",
        "    ],\n",
        "    \"up_blocks\" : [\n",
        "        [512,512],\n",
        "        [256,256],\n",
        "        [128,128],\n",
        "    ]\n",
        "}\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import dataloader\n",
        "from torchvision.utils import make_grid\n",
        "# Define the transformation to apply to the dataset\n",
        "W,H = 32,32\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((W,H)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "# Download the CIFAR10 dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "# trainset = torchvision.datasets.CelebA(root='./data', split='train', download=True, transform=transform)\n",
        "# trainset = torchvision.datasets.ImageFolder(\"dataset\",transform=transform)\n",
        "batch_size = 10 \n",
        "trainloader = dataloader.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "device = torch.device('cuda')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "dFCIVw22Xh5s",
        "outputId": "2f3d859c-77f0-4510-f660-3ea609a25f06"
      },
      "outputs": [],
      "source": [
        "\n",
        "# model = DDPM(Config).to(device)\n",
        "# # print(f\"running on {device}\")\n",
        "# x,_ = next(iter(trainloader))\n",
        "# img = make_grid(x)\n",
        "# plt.imshow(img.permute(1,2,0).detach().to(\"cpu\").numpy() / 2 + 0.5)\n",
        "# x = x.to(device)\n",
        "# nosie = model.gen_noise_img(x,500)\n",
        "# noise2 = model.gen_noise_img(x,999)\n",
        "# nosie_gen = torch.randn_like(x)\n",
        "# print(nosie.mean())\n",
        "# print(nosie.var())\n",
        "# print(noise2.mean())\n",
        "# print(noise2.var())\n",
        "# print(nosie_gen.mean())\n",
        "# print(nosie_gen.var())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535,
          "referenced_widgets": [
            "c2a895c3c47349819450dca7d70071e4",
            "03899c7b511545c7913bd669d8d9fb59",
            "0b00e18c881d4968add1764c03e62aca",
            "a14c974c9fae487bab6cd555464b053c",
            "98b27000f6a249ac842b64cd488af9c4",
            "9da0a0fac79a4f06903c1e96ec8d592f",
            "687adfca0e2746b394ce9c13586b89f3",
            "292b1b6835ff4660a8be5066125f7462",
            "642ad21483e64ff3abba1cb4aa1f3342",
            "bfd029fe51cb45e0a5c93712bd86c422",
            "a0b57c1aba3c42ea8f498f78dbd4bc29"
          ]
        },
        "id": "f7Q1pnRzFaPO",
        "outputId": "a9676cc9-cc3b-44c9-c7e7-e8c469115e5c"
      },
      "outputs": [],
      "source": [
        "import lightning as L\n",
        "class DDPM_lightning(L.LightningModule):\n",
        "    def __init__(self,Config):\n",
        "        super().__init__()\n",
        "        self.model = DDPM(Config)\n",
        "    def forward(self,x):\n",
        "        return self.model(x)\n",
        "    def training_step(self,batch,batch_idx):\n",
        "        x,_ = batch\n",
        "        loss = self.model(x)\n",
        "        self.log('train_loss',loss,prog_bar=True)\n",
        "        return loss\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(),lr=2e-5)\n",
        "    def train_dataloader(self):\n",
        "        return trainloader\n",
        "    def sample(self,return_progress=False,step=None):\n",
        "        device =  torch.device(\"cuda\")\n",
        "        x = torch.randn((4,3,32,32)).to(device)\n",
        "        return self.model.sample(x,return_progress,step)\n",
        "# CKPT_PATH = r\"lightning_logs\\version_2\\checkpoints\\epoch=3-step=20000.ckpt\"\n",
        "# model = DDPM_lightning.load_from_checkpoint(CKPT_PATH,Config=Config)\n",
        "# trainer = L.Trainer(max_epochs=4)\n",
        "# model = DDPM_lightning(Config)\n",
        "# trainer.fit(model,trainloader)\n",
        "# trainer.predict(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision.utils import make_grid\n",
        "CKPT_PATH = r\"lightning_logs\\colab\\v2epoch=7-step=4000.ckpt\"\n",
        "device = torch.device(\"cuda\")\n",
        "model = DDPM_lightning.load_from_checkpoint(CKPT_PATH,Config=Config)\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "qU4uAeuudQgY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.6017, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "x_0,_= next(iter(trainloader))\n",
        "x_0 = x_0.to(device)\n",
        "t = 1\n",
        "noisy_img,noise = model.model.gen_noise_img(x_0,t)\n",
        "\n",
        "noise2 = torch.randn_like(x_0)\n",
        "predicted_noise = model.model.unet(noisy_img,torch.full((10,),t).to(device))\n",
        "\n",
        "loss = nn.functional.mse_loss(predicted_noise,noise)\n",
        "loss2 = nn.functional.mse_loss(noise2,noise)\n",
        "print(loss)\n",
        "# img = make_grid(model.sample(return_progress=True,step=100)).detach().to(\"cpu\").permute((1,2,0)).numpy() / 2 + 0.5\n",
        "# plt.imshow(img)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.9908, device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "print(loss2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.0100, 0.0081, 0.0074, 0.0070, 0.0068, 0.0067, 0.0066, 0.0065, 0.0065,\n",
            "        0.0064, 0.0064, 0.0064, 0.0064, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063,\n",
            "        0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063,\n",
            "        0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063,\n",
            "        0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063,\n",
            "        0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063,\n",
            "        0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063,\n",
            "        0.0064, 0.0064, 0.0064, 0.0064, 0.0064, 0.0064, 0.0064, 0.0064, 0.0064,\n",
            "        0.0064, 0.0064, 0.0064, 0.0064, 0.0064, 0.0064, 0.0064, 0.0064, 0.0064,\n",
            "        0.0064, 0.0064, 0.0064, 0.0064, 0.0064, 0.0064, 0.0064, 0.0064, 0.0064,\n",
            "        0.0064, 0.0064, 0.0064, 0.0064, 0.0064, 0.0064, 0.0064, 0.0065, 0.0065,\n",
            "        0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
            "        0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065,\n",
            "        0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0065, 0.0066, 0.0066,\n",
            "        0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
            "        0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066, 0.0066,\n",
            "        0.0066, 0.0066, 0.0067, 0.0067, 0.0067, 0.0067, 0.0067, 0.0067, 0.0067,\n",
            "        0.0067, 0.0067, 0.0067, 0.0067, 0.0067, 0.0067, 0.0067, 0.0067, 0.0067,\n",
            "        0.0067, 0.0067, 0.0067, 0.0068, 0.0068, 0.0068, 0.0068, 0.0068, 0.0068,\n",
            "        0.0068, 0.0068, 0.0068, 0.0068, 0.0068, 0.0068, 0.0068, 0.0068, 0.0068,\n",
            "        0.0068, 0.0068, 0.0069, 0.0069, 0.0069, 0.0069, 0.0069, 0.0069, 0.0069,\n",
            "        0.0069, 0.0069, 0.0069, 0.0069, 0.0069, 0.0069, 0.0069, 0.0069, 0.0069,\n",
            "        0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070,\n",
            "        0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0070, 0.0071, 0.0071, 0.0071,\n",
            "        0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071, 0.0071,\n",
            "        0.0071, 0.0072, 0.0072, 0.0072, 0.0072, 0.0072, 0.0072, 0.0072, 0.0072,\n",
            "        0.0072, 0.0072, 0.0072, 0.0072, 0.0072, 0.0073, 0.0073, 0.0073, 0.0073,\n",
            "        0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0073, 0.0074,\n",
            "        0.0074, 0.0074, 0.0074, 0.0074, 0.0074, 0.0074, 0.0074, 0.0074, 0.0074,\n",
            "        0.0074, 0.0074, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075,\n",
            "        0.0075, 0.0075, 0.0075, 0.0075, 0.0076, 0.0076, 0.0076, 0.0076, 0.0076,\n",
            "        0.0076, 0.0076, 0.0076, 0.0076, 0.0076, 0.0076, 0.0077, 0.0077, 0.0077,\n",
            "        0.0077, 0.0077, 0.0077, 0.0077, 0.0077, 0.0077, 0.0077, 0.0078, 0.0078,\n",
            "        0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0079,\n",
            "        0.0079, 0.0079, 0.0079, 0.0079, 0.0079, 0.0079, 0.0079, 0.0079, 0.0079,\n",
            "        0.0080, 0.0080, 0.0080, 0.0080, 0.0080, 0.0080, 0.0080, 0.0080, 0.0080,\n",
            "        0.0081, 0.0081, 0.0081, 0.0081, 0.0081, 0.0081, 0.0081, 0.0081, 0.0081,\n",
            "        0.0082, 0.0082, 0.0082, 0.0082, 0.0082, 0.0082, 0.0082, 0.0082, 0.0082,\n",
            "        0.0083, 0.0083, 0.0083, 0.0083, 0.0083, 0.0083, 0.0083, 0.0083, 0.0084,\n",
            "        0.0084, 0.0084, 0.0084, 0.0084, 0.0084, 0.0084, 0.0084, 0.0084, 0.0085,\n",
            "        0.0085, 0.0085, 0.0085, 0.0085, 0.0085, 0.0085, 0.0085, 0.0086, 0.0086,\n",
            "        0.0086, 0.0086, 0.0086, 0.0086, 0.0086, 0.0086, 0.0087, 0.0087, 0.0087,\n",
            "        0.0087, 0.0087, 0.0087, 0.0087, 0.0087, 0.0088, 0.0088, 0.0088, 0.0088,\n",
            "        0.0088, 0.0088, 0.0088, 0.0089, 0.0089, 0.0089, 0.0089, 0.0089, 0.0089,\n",
            "        0.0089, 0.0089, 0.0090, 0.0090, 0.0090, 0.0090, 0.0090, 0.0090, 0.0090,\n",
            "        0.0091, 0.0091, 0.0091, 0.0091, 0.0091, 0.0091, 0.0091, 0.0091, 0.0092,\n",
            "        0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0092, 0.0093, 0.0093, 0.0093,\n",
            "        0.0093, 0.0093, 0.0093, 0.0093, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
            "        0.0094, 0.0094, 0.0095, 0.0095, 0.0095, 0.0095, 0.0095, 0.0095, 0.0095,\n",
            "        0.0096, 0.0096, 0.0096, 0.0096, 0.0096, 0.0096, 0.0096, 0.0097, 0.0097,\n",
            "        0.0097, 0.0097, 0.0097, 0.0097, 0.0098, 0.0098, 0.0098, 0.0098, 0.0098,\n",
            "        0.0098, 0.0098, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0099, 0.0100,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0101, 0.0101, 0.0101,\n",
            "        0.0101, 0.0101, 0.0101, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102, 0.0102,\n",
            "        0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0103, 0.0104, 0.0104,\n",
            "        0.0104, 0.0104, 0.0104, 0.0104, 0.0105, 0.0105, 0.0105, 0.0105, 0.0105,\n",
            "        0.0105, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0106, 0.0107, 0.0107,\n",
            "        0.0107, 0.0107, 0.0107, 0.0107, 0.0108, 0.0108, 0.0108, 0.0108, 0.0108,\n",
            "        0.0108, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0110, 0.0110,\n",
            "        0.0110, 0.0110, 0.0110, 0.0110, 0.0111, 0.0111, 0.0111, 0.0111, 0.0111,\n",
            "        0.0111, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0112, 0.0113, 0.0113,\n",
            "        0.0113, 0.0113, 0.0113, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114, 0.0114,\n",
            "        0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0115, 0.0116, 0.0116, 0.0116,\n",
            "        0.0116, 0.0116, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0117, 0.0118,\n",
            "        0.0118, 0.0118, 0.0118, 0.0118, 0.0118, 0.0119, 0.0119, 0.0119, 0.0119,\n",
            "        0.0119, 0.0120, 0.0120, 0.0120, 0.0120, 0.0120, 0.0120, 0.0121, 0.0121,\n",
            "        0.0121, 0.0121, 0.0121, 0.0122, 0.0122, 0.0122, 0.0122, 0.0122, 0.0122,\n",
            "        0.0123, 0.0123, 0.0123, 0.0123, 0.0123, 0.0124, 0.0124, 0.0124, 0.0124,\n",
            "        0.0124, 0.0124, 0.0125, 0.0125, 0.0125, 0.0125, 0.0125, 0.0126, 0.0126,\n",
            "        0.0126, 0.0126, 0.0126, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127, 0.0127,\n",
            "        0.0128, 0.0128, 0.0128, 0.0128, 0.0128, 0.0129, 0.0129, 0.0129, 0.0129,\n",
            "        0.0129, 0.0129, 0.0130, 0.0130, 0.0130, 0.0130, 0.0130, 0.0131, 0.0131,\n",
            "        0.0131, 0.0131, 0.0131, 0.0132, 0.0132, 0.0132, 0.0132, 0.0132, 0.0133,\n",
            "        0.0133, 0.0133, 0.0133, 0.0133, 0.0133, 0.0134, 0.0134, 0.0134, 0.0134,\n",
            "        0.0134, 0.0135, 0.0135, 0.0135, 0.0135, 0.0135, 0.0136, 0.0136, 0.0136,\n",
            "        0.0136, 0.0136, 0.0137, 0.0137, 0.0137, 0.0137, 0.0137, 0.0137, 0.0138,\n",
            "        0.0138, 0.0138, 0.0138, 0.0138, 0.0139, 0.0139, 0.0139, 0.0139, 0.0139,\n",
            "        0.0140, 0.0140, 0.0140, 0.0140, 0.0140, 0.0141, 0.0141, 0.0141, 0.0141,\n",
            "        0.0141, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0142, 0.0143, 0.0143,\n",
            "        0.0143, 0.0143, 0.0143, 0.0144, 0.0144, 0.0144, 0.0144, 0.0144, 0.0145,\n",
            "        0.0145, 0.0145, 0.0145, 0.0145, 0.0146, 0.0146, 0.0146, 0.0146, 0.0146,\n",
            "        0.0147, 0.0147, 0.0147, 0.0147, 0.0147, 0.0148, 0.0148, 0.0148, 0.0148,\n",
            "        0.0148, 0.0148, 0.0149, 0.0149, 0.0149, 0.0149, 0.0149, 0.0150, 0.0150,\n",
            "        0.0150, 0.0150, 0.0150, 0.0151, 0.0151, 0.0151, 0.0151, 0.0151, 0.0152,\n",
            "        0.0152, 0.0152, 0.0152, 0.0152, 0.0153, 0.0153, 0.0153, 0.0153, 0.0153,\n",
            "        0.0154, 0.0154, 0.0154, 0.0154, 0.0154, 0.0155, 0.0155, 0.0155, 0.0155,\n",
            "        0.0155, 0.0156, 0.0156, 0.0156, 0.0156, 0.0156, 0.0157, 0.0157, 0.0157,\n",
            "        0.0157, 0.0157, 0.0158, 0.0158, 0.0158, 0.0158, 0.0158, 0.0159, 0.0159,\n",
            "        0.0159, 0.0159, 0.0159, 0.0159, 0.0160, 0.0160, 0.0160, 0.0160, 0.0160,\n",
            "        0.0161, 0.0161, 0.0161, 0.0161, 0.0161, 0.0162, 0.0162, 0.0162, 0.0162,\n",
            "        0.0162, 0.0163, 0.0163, 0.0163, 0.0163, 0.0163, 0.0164, 0.0164, 0.0164,\n",
            "        0.0164, 0.0164, 0.0165, 0.0165, 0.0165, 0.0165, 0.0165, 0.0166, 0.0166,\n",
            "        0.0166, 0.0166, 0.0166, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0168,\n",
            "        0.0168, 0.0168, 0.0168, 0.0168, 0.0169, 0.0169, 0.0169, 0.0169, 0.0169,\n",
            "        0.0170, 0.0170, 0.0170, 0.0170, 0.0170, 0.0171, 0.0171, 0.0171, 0.0171,\n",
            "        0.0171, 0.0172, 0.0172, 0.0172, 0.0172, 0.0172, 0.0173, 0.0173, 0.0173,\n",
            "        0.0173, 0.0173, 0.0174, 0.0174, 0.0174, 0.0174, 0.0174, 0.0175, 0.0175,\n",
            "        0.0175, 0.0175, 0.0175, 0.0176, 0.0176, 0.0176, 0.0176, 0.0176, 0.0177,\n",
            "        0.0177, 0.0177, 0.0177, 0.0177, 0.0178, 0.0178, 0.0178, 0.0178, 0.0178,\n",
            "        0.0179, 0.0179, 0.0179, 0.0179, 0.0179, 0.0180, 0.0180, 0.0180, 0.0180,\n",
            "        0.0180, 0.0181, 0.0181, 0.0181, 0.0181, 0.0181, 0.0181, 0.0182, 0.0182,\n",
            "        0.0182, 0.0182, 0.0182, 0.0183, 0.0183, 0.0183, 0.0183, 0.0183, 0.0184,\n",
            "        0.0184, 0.0184, 0.0184, 0.0184, 0.0185, 0.0185, 0.0185, 0.0185, 0.0185,\n",
            "        0.0186, 0.0186, 0.0186, 0.0186, 0.0186, 0.0187, 0.0187, 0.0187, 0.0187,\n",
            "        0.0187, 0.0188, 0.0188, 0.0188, 0.0188, 0.0188, 0.0189, 0.0189, 0.0189,\n",
            "        0.0189, 0.0189, 0.0190, 0.0190, 0.0190, 0.0190, 0.0190, 0.0191, 0.0191,\n",
            "        0.0191, 0.0191, 0.0191, 0.0192, 0.0192, 0.0192, 0.0192, 0.0192, 0.0193,\n",
            "        0.0193, 0.0193, 0.0193, 0.0193, 0.0194, 0.0194, 0.0194, 0.0194, 0.0194,\n",
            "        0.0195, 0.0195, 0.0195, 0.0195, 0.0195, 0.0196, 0.0196, 0.0196, 0.0196,\n",
            "        0.0196, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0198, 0.0198, 0.0198,\n",
            "        0.0198, 0.0198, 0.0199, 0.0199, 0.0199, 0.0199, 0.0199, 0.0200, 0.0200,\n",
            "        0.0200], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "print(model.model.noise_coeff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zAPqVopdLE07"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Launching TensorBoard..."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-be6d43057c0d894d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-be6d43057c0d894d\");\n",
              "          const url = new URL(\"http://localhost\");\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir=lightning_logs/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "03899c7b511545c7913bd669d8d9fb59": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9da0a0fac79a4f06903c1e96ec8d592f",
            "placeholder": "​",
            "style": "IPY_MODEL_687adfca0e2746b394ce9c13586b89f3",
            "value": "Epoch 0:  61%"
          }
        },
        "0b00e18c881d4968add1764c03e62aca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_292b1b6835ff4660a8be5066125f7462",
            "max": 625,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_642ad21483e64ff3abba1cb4aa1f3342",
            "value": 380
          }
        },
        "292b1b6835ff4660a8be5066125f7462": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "642ad21483e64ff3abba1cb4aa1f3342": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "687adfca0e2746b394ce9c13586b89f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98b27000f6a249ac842b64cd488af9c4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "9da0a0fac79a4f06903c1e96ec8d592f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0b57c1aba3c42ea8f498f78dbd4bc29": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a14c974c9fae487bab6cd555464b053c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfd029fe51cb45e0a5c93712bd86c422",
            "placeholder": "​",
            "style": "IPY_MODEL_a0b57c1aba3c42ea8f498f78dbd4bc29",
            "value": " 380/625 [05:19&lt;03:26,  1.19it/s, v_num=6, train_loss=0.0814]"
          }
        },
        "bfd029fe51cb45e0a5c93712bd86c422": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2a895c3c47349819450dca7d70071e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_03899c7b511545c7913bd669d8d9fb59",
              "IPY_MODEL_0b00e18c881d4968add1764c03e62aca",
              "IPY_MODEL_a14c974c9fae487bab6cd555464b053c"
            ],
            "layout": "IPY_MODEL_98b27000f6a249ac842b64cd488af9c4"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
